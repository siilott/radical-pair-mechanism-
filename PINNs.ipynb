{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import functools\n",
    "import numpy as np\n",
    "import scipy as sci\n",
    "import matplotlib.pyplot as plt\n",
    "import qutip as qt\n",
    "import time\n",
    "import operator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This dictionary maps string keys ('x', 'y', 'z', 'p', 'm', 'i') to functions that generate spin operators for a given dimension dim.\n",
    "# opstr2fun = {'x': lambda dim: qt.spin_Jx((dim-1)/2),\n",
    "#              'y': lambda dim: qt.spin_Jy((dim-1)/2),\n",
    "#              'z': lambda dim: qt.spin_Jz((dim-1)/2),\n",
    "#              'p': lambda dim: qt.spin_Jp((dim-1)/2),\n",
    "#              'm': lambda dim: qt.spin_Jm((dim-1)/2),\n",
    "#              'i': qt.identity}\n",
    "# # Initializes ops as a list of identity matrices for each dimension in dims. Iterates over specs to replace the identity matrix at the specified index with the corresponding spin operator. Returns the tensor product of the operators in ops using qt.tensor.\n",
    "# def mkSpinOp(dims, specs):\n",
    "#     ops = [qt.identity(d) for d in dims]\n",
    "#     for ind, opstr in specs:\n",
    "#         ops[ind] = ops[ind] * opstr2fun[opstr](dims[ind])\n",
    "#     return qt.tensor(ops)\n",
    "# # Constructs a Hamiltonian for a single spin system with interactions along the x, y, and z axes.\n",
    "# def mkH1(dims, ind, parvec):\n",
    "#     axes = ['x', 'y', 'z']\n",
    "#     # Creates a list of spin operators weighted by the corresponding parameters in parvec (ignores zero parameters). Uses functools.reduce to sum these weighted spin operators.\n",
    "#     return functools.reduce(lambda a, b: a + b, \n",
    "#                [v * mkSpinOp(dims, [(ind,ax)]) for v, ax in zip(parvec, axes) if v!=0])\n",
    "# # Constructs a Hamiltonian for the interaction between two spin systems with interaction terms along all combinations of x, y, and z axes.\n",
    "# def mkH12(dims, ind1, ind2, parmat):\n",
    "#     axes = ['x', 'y', 'z']\n",
    "#     ops = []\n",
    "#     # Iterates over all combinations of the x, y, and z axes for the two spins. For each non-zero element in parmat, adds the corresponding spin-spin interaction term to the empty list ops.\n",
    "#     for i in range(3):\n",
    "#         for j in range(3):\n",
    "#             if parmat[i,j] != 0:\n",
    "#                 ops.append(parmat[i,j] * mkSpinOp(dims, [(ind1,axes[i]), (ind2,axes[j])]))\n",
    "#     return functools.reduce(lambda a, b: a + b, ops) # Uses functools.reduce to sum these interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N5_C =  2*np.pi* np.array([[-0.36082693, -0.0702137 , -1.41518116],\n",
    "#       [-0.0702137 , -0.60153649,  0.32312139],\n",
    "#       [-1.41518116,  0.32312139, 50.80213093]]) # in MHz\n",
    "\t  \n",
    "# N1_C = 2*np.pi*np.array([[  2.13814981,   3.19255832,  -2.48895215],\n",
    "#       [  3.19255832,  15.45032887, -12.44778343],\n",
    "#       [ -2.48895215, -12.44778343,  12.49532827]]) # in MHz\n",
    "\n",
    "# N5_D =  2*np.pi*np.array([[-2.94412424e-01, -5.68059200e-02, -1.02860888e+00],\n",
    "#       [-5.68059200e-02, -5.40578469e-01, -2.67686240e-02],\n",
    "#       [-1.02860888e+00, -2.67686240e-02,  5.05815320e+01]]) # in MHz\n",
    "\t  \n",
    "# N1_D = 2*np.pi* np.array([[ 0.98491908,  3.28010265, -0.53784491],\n",
    "#       [ 3.28010265, 25.88547678, -1.6335986 ],\n",
    "#       [-0.53784491, -1.6335986 ,  1.41368001]]) # in MHz\n",
    "\n",
    "\n",
    "# ErC_Dee =  np.array([[ 26.47042689, -55.90357828,  50.1679204 ],\n",
    "#                             [-55.90357828, -20.86385225,  76.13493805],\n",
    "#                              [ 50.1679204,  76.13493805,  -5.60657464]]) # in Mrad/s\n",
    "\n",
    "\n",
    "\n",
    "# ErD_Dee = np.array([[ 11.08087889, -34.6687169,   12.14623706],\n",
    "#                             [-34.6687169,  -33.09039672,  22.36229081],\n",
    "#                             [ 12.14623706,  22.36229081,  22.00951783]]) #  in Mrad/s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b0 = 1.4 * 2*math.pi # Zeeman field strength in radians per microsecond\n",
    "\n",
    "# krA = 1 # Rate constant 1/us\n",
    "# krB = 0.1\n",
    "# kf = 1\n",
    "# kAB = 1e4\n",
    "# kBA = 1e3\n",
    "\n",
    "# tmax = 12. / krA # Maximum time us \n",
    "# tlist = np.linspace(0, tmax, math.ceil(1000*tmax)) # Time points for simulation\n",
    "\n",
    "# theta = np.linspace(0, np.pi, 15)\n",
    "\n",
    "# xyz = []\n",
    "\n",
    "# for theta_ in theta:\n",
    "#     x = np.sin(theta_)\n",
    "#     y = 0\n",
    "#     z = np.cos(theta_)\n",
    "#     xyz.append([x, y, z])       \n",
    "\n",
    "# oris = np.asarray(xyz)\n",
    "\n",
    "# # Initialize arrays for latitude and longitude\n",
    "# num_points = len(oris)\n",
    "# lat = np.zeros(num_points)\n",
    "# lon = np.zeros(num_points)\n",
    "\n",
    "# # Convert Cartesian coordinates to latitude and longitude\n",
    "# for i in range(num_points):\n",
    "#     x, y, z = oris[i]\n",
    "#     lat[i] = np.arcsin(z) * (180/np.pi)\n",
    "#     lon[i] = np.arctan2(y, x) * (180/np.pi)\n",
    "\n",
    "# dims = [2, 2, 3, 3, 3] # Dimensions of the system components (2 qubits, 1 spin-1 nucleus)\n",
    "# dim = np.prod(dims) # Total dimension of the composite system\n",
    "\n",
    "# H_C_list = []\n",
    "# H_D_list = []\n",
    "\n",
    "# for orientation in oris:\n",
    "#     B0 = b0 * orientation  # Magnetic field vector along orientation\n",
    "\n",
    "#     # Compute Hamiltonians for each orientation\n",
    "#     Hzee = mkH1(dims, 0, B0) + mkH1(dims, 1, B0)  # Zeeman Hamiltonian for two spins\n",
    "#     Hhfc_C = mkH12(dims, 0, 2, N5_C) + mkH12(dims, 1, 3, N1_C)\n",
    "#     Hhfc_D = mkH12(dims, 0, 2, N5_D) + mkH12(dims, 1, 4, N1_D)\n",
    "#     Hdee_C = mkH12(dims, 0, 1, ErC_Dee)\n",
    "#     Hdee_D = mkH12(dims, 0, 1, ErD_Dee)\n",
    "#     H0_C = Hzee + Hhfc_C + Hdee_C  # Total Hamiltonian for component C\n",
    "#     H0_D = Hzee + Hhfc_D + Hdee_D  # Total Hamiltonian for component D\n",
    "    \n",
    "#     # Append Hamiltonians to the list\n",
    "#     H_C_list.append(H0_C.data)\n",
    "#     H_D_list.append(H0_D.data)\n",
    "\n",
    "# # Now H_C_list and H_D_list contain Hamiltonians for each orientation\n",
    "\n",
    "# Ps = 1/4 * mkSpinOp(dims,[]) - mkH12(dims, 0, 1, np.identity(3)) # Singlet projection operator\n",
    "\n",
    "\n",
    "# rho0_C = (Ps / Ps.tr()).full().flatten()# Initial density matrix, normalized projection operator for the singlet state.\n",
    "# rho0_D = np.zeros_like(rho0_C)\n",
    "# Ps = Ps.data\n",
    "\n",
    "# # Combine the initial states into one vector\n",
    "# initial_state = np.concatenate((rho0_C, rho0_D)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 154.5664,    0.0000,    0.0000,    4.3982,    0.0000,    0.0000,\n",
      "            4.3982,    0.0000,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    4.3982,    0.0000,\n",
      "          -11.5515,    4.3982,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000, -154.5664,    0.0000,    0.0000,    4.3982,\n",
      "            0.0000,  -11.5515,    4.3982,    0.0000,    0.0000,    0.0000],\n",
      "        [   4.3982,    0.0000,    0.0000,  154.5664,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    4.3982,    0.0000,    0.0000],\n",
      "        [   0.0000,    4.3982,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,  -11.5515,    4.3982,    0.0000],\n",
      "        [   0.0000,    0.0000,    4.3982,    0.0000,    0.0000, -154.5664,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,  -11.5515,    4.3982],\n",
      "        [   4.3982,  -11.5515,    0.0000,    0.0000,    0.0000,    0.0000,\n",
      "         -154.5664,    0.0000,    0.0000,    4.3982,    0.0000,    0.0000],\n",
      "        [   0.0000,    4.3982,  -11.5515,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,    0.0000,    0.0000,    4.3982,    0.0000],\n",
      "        [   0.0000,    0.0000,    4.3982,    0.0000,    0.0000,    0.0000,\n",
      "            0.0000,    0.0000,  154.5664,    0.0000,    0.0000,    4.3982],\n",
      "        [   0.0000,    0.0000,    0.0000,    4.3982,  -11.5515,    0.0000,\n",
      "            4.3982,    0.0000,    0.0000, -154.5664,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    4.3982,  -11.5515,\n",
      "            0.0000,    4.3982,    0.0000,    0.0000,    0.0000,    0.0000],\n",
      "        [   0.0000,    0.0000,    0.0000,    0.0000,    0.0000,    4.3982,\n",
      "            0.0000,    0.0000,    4.3982,    0.0000,    0.0000,  154.5664]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# # Convert the Hamiltonian H0 to a scipy sparse matrix and then to a PyTorch sparse tensor\n",
    "# H_scipy = H0.data.tocsc()  # Convert to a SciPy sparse CSC matrix\n",
    "\n",
    "# # Extract the indices and values for the sparse tensor\n",
    "# H_indices = torch.LongTensor(np.vstack([H_scipy.nonzero()[0], H_scipy.nonzero()[1]]))  # Non-zero element indices\n",
    "# H_values = torch.FloatTensor(H_scipy.data)  # Non-zero values\n",
    "# H_shape = H_scipy.shape  # Shape of the matrix\n",
    "\n",
    "# # Convert to a PyTorch sparse tensor\n",
    "# # H_torch_sparse = torch.sparse_coo_tensor(H_indices, H_values, H_shape, requires_grad=True).to(device)\n",
    "# H_torch = torch.tensor(H_scipy.toarray(), dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# Ps = Ps.data.toarray()\n",
    "# print(H_torch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device and data type\n",
    "dtype = torch.float\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# Define the neural network architecture\n",
    "dimA, dimB = 2, 2  # Dimensions of subsystem A and B (for example 2x2 matrices)\n",
    "ns = [20, 20, 20, 20]\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(1, ns[0]),\n",
    "    nn.Sigmoid(),\n",
    "    *functools.reduce(operator.iconcat, [(nn.Linear(*d), nn.Sigmoid()) for d in zip(ns[:-1], ns[1:])], []),\n",
    "    nn.Linear(ns[-1], 2*(dimA*dimA + dimB*dimB))  # Output real and imaginary parts for both density matrices\n",
    ")\n",
    "\n",
    "# Function to split the real and imaginary parts of the neural network output\n",
    "def split_real_imag(u, dimA, dimB):\n",
    "    lenA = dimA * dimA\n",
    "    lenB = dimB * dimB\n",
    "    rhoA_real = u[:, :lenA]\n",
    "    rhoA_imag = u[:, lenA:2*lenA]\n",
    "    rhoB_real = u[:, 2*lenA:2*lenA+lenB]\n",
    "    rhoB_imag = u[:, 2*lenA+lenB:]\n",
    "    return rhoA_real, rhoA_imag, rhoB_real, rhoB_imag\n",
    "\n",
    "# Define the master equation function\n",
    "def mesolve(t, u, P_s, HA, HB, krA, krB, kAB, kBA, kf, dimA, dimB):\n",
    "    # Extract real and imaginary parts from the neural network output\n",
    "    rhoA_real, rhoA_imag, rhoB_real, rhoB_imag = split_real_imag(u, dimA, dimB)\n",
    "    \n",
    "    # Reshape real and imaginary parts into matrix form\n",
    "    rhoA_real = rhoA_real.reshape(-1, dimA, dimA)\n",
    "    rhoA_imag = rhoA_imag.reshape(-1, dimA, dimA)\n",
    "    rhoB_real = rhoB_real.reshape(-1, dimB, dimB)\n",
    "    rhoB_imag = rhoB_imag.reshape(-1, dimB, dimB)\n",
    "\n",
    "    # Separate real and imaginary components of Hamiltonian evolution: \n",
    "    # HA @ rhoA - rhoA @ HA and HB @ rhoB - rhoB @ HB\n",
    "    HA_rhoA_real = HA @ rhoA_real - rhoA_real @ HA - rhoA_imag @ HA + HA @ rhoA_imag\n",
    "    HA_rhoA_imag = HA @ rhoA_imag - rhoA_imag @ HA + rhoA_real @ HA - HA @ rhoA_real\n",
    "    HB_rhoB_real = HB @ rhoB_real - rhoB_real @ HB - rhoB_imag @ HB + HB @ rhoB_imag\n",
    "    HB_rhoB_imag = HB @ rhoB_imag - rhoB_imag @ HB + rhoB_real @ HB - HB @ rhoB_real\n",
    "    \n",
    "    # For the Lindblad terms (-kr * (P_s @ rho + rho @ P_s)/2)\n",
    "    Ps_rhoA_real = P_s @ rhoA_real + rhoA_real @ P_s\n",
    "    Ps_rhoA_imag = P_s @ rhoA_imag + rhoA_imag @ P_s\n",
    "    Ps_rhoB_real = P_s @ rhoB_real + rhoB_real @ P_s\n",
    "    Ps_rhoB_imag = P_s @ rhoB_imag + rhoB_imag @ P_s\n",
    "\n",
    "    # drhoA/dt for real and imaginary parts\n",
    "    drhoA_real = -HA_rhoA_imag - krA * Ps_rhoA_real / 2 - (kAB + kf) * rhoA_real + kBA * rhoB_real\n",
    "    drhoA_imag = HA_rhoA_real - krA * Ps_rhoA_imag / 2 - (kAB + kf) * rhoA_imag + kBA * rhoB_imag\n",
    "    \n",
    "    # drhoB/dt for real and imaginary parts\n",
    "    drhoB_real = -HB_rhoB_imag - krB * Ps_rhoB_real / 2 - (kBA + kf) * rhoB_real + kAB * rhoA_real\n",
    "    drhoB_imag = HB_rhoB_real - krB * Ps_rhoB_imag / 2 - (kBA + kf) * rhoB_imag + kAB * rhoA_imag\n",
    "\n",
    "    # Return concatenated real and imaginary parts as a flat vector\n",
    "    return torch.cat((drhoA_real.flatten(), drhoA_imag.flatten(), drhoB_real.flatten(), drhoB_imag.flatten()), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dudt function\n",
    "def generate_dudt(HA, HB, P_s, krA, krB, kAB, kBA, kf, dimA, dimB):\n",
    "    def dudt(u):\n",
    "        return mesolve(None, u, P_s, HA, HB, krA, krB, kAB, kBA, kf, dimA, dimB)\n",
    "    return dudt\n",
    "\n",
    "# Generate a model for time evolution\n",
    "def generate_model(net, u0):\n",
    "    def fun(t):\n",
    "        return torch.add(torch.mul(torch.add(torch.ones(1), torch.neg(torch.exp(torch.neg(t)))), net(t)), u0)\n",
    "    return fun\n",
    "\n",
    "# Define the loss function (to minimize)\n",
    "def total_loss(u, u_t, u_t_exact, scale_norm):\n",
    "    mse_dudt = nn.MSELoss()(u_t, u_t_exact)\n",
    "    mse_norm = torch.mean((torch.norm(u, dim=1)-1)**2)\n",
    "    return mse_dudt + scale_norm * mse_norm\n",
    "\n",
    "# Generate loss function using autograd\n",
    "def generate_loss(model, dudt, p=0):\n",
    "    def fun(t):\n",
    "        u = model(t)\n",
    "        u_t = torch.autograd.functional.jacobian(lambda t_: model(t_).sum(axis=0), t, create_graph=True).squeeze(-1).T\n",
    "        u_t_exact = dudt(u)\n",
    "        return total_loss(u, u_t, u_t_exact, p)\n",
    "    return fun\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Hamiltonians and parameters (change these values based on your system)\n",
    "HA = torch.tensor([[0, 1], [1, 0]], dtype=dtype, device=device)  # Example 2x2 Hamiltonian for A\n",
    "HB = torch.tensor([[0, 1], [1, 0]], dtype=dtype, device=device)  # Example 2x2 Hamiltonian for B\n",
    "P_s = torch.tensor([[1, 0], [0, 0]], dtype=dtype, device=device)  # Some projection operator\n",
    "krA, krB, kAB, kBA, kf = 0.1, 0.1, 0.2, 0.2, 0.05  # Example rates\n",
    "\n",
    "# Generate the dudt function\n",
    "dudt = generate_dudt(HA, HB, P_s, krA, krB, kAB, kBA, kf, dimA, dimB)\n",
    "\n",
    "# Initial state (u0) in real and imaginary parts\n",
    "u0_real = torch.randn(1, dimA*dimA + dimB*dimB, device=device)  # Initial real part\n",
    "u0_imag = torch.randn(1, dimA*dimA + dimB*dimB, device=device)  # Initial imaginary part\n",
    "u0 = torch.cat((u0_real, u0_imag), dim=1)\n",
    "\n",
    "# Generate the model function for time evolution\n",
    "model = generate_model(net, u0)\n",
    "\n",
    "# Time points\n",
    "t = torch.linspace(0, 10, 100, dtype=dtype, device=device).unsqueeze(-1)\n",
    "\n",
    "# Generate the loss function\n",
    "loss_fun = generate_loss(model, dudt)\n",
    "\n",
    "# Optimizer for training\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sajai/opt/anaconda3/lib/python3.9/site-packages/torch/utils/_device.py:78: UserWarning: Using a target size (torch.Size([1600])) that is different to the input size (torch.Size([100, 16])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (16) must match the size of tensor b (1600) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [88]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      4\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 5\u001b[0m     loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     loss_value\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "Input \u001b[0;32mIn [86]\u001b[0m, in \u001b[0;36mgenerate_loss.<locals>.fun\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     23\u001b[0m u_t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mjacobian(\u001b[38;5;28;01mlambda\u001b[39;00m t_: model(t_)\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), t, create_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m     24\u001b[0m u_t_exact \u001b[38;5;241m=\u001b[39m dudt(u)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtotal_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_t_exact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [86]\u001b[0m, in \u001b[0;36mtotal_loss\u001b[0;34m(u, u_t, u_t_exact, scale_norm)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtotal_loss\u001b[39m(u, u_t, u_t_exact, scale_norm):\n\u001b[0;32m---> 15\u001b[0m     mse_dudt \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMSELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_t_exact\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     mse_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((torch\u001b[38;5;241m.\u001b[39mnorm(u, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mse_dudt \u001b[38;5;241m+\u001b[39m scale_norm \u001b[38;5;241m*\u001b[39m mse_norm\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:3352\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"mse_loss(input, target, size_average=None, reduce=None, reduction='mean') -> Tensor\u001b[39;00m\n\u001b[1;32m   3347\u001b[0m \n\u001b[1;32m   3348\u001b[0m \u001b[38;5;124;03mMeasures the element-wise mean squared error.\u001b[39;00m\n\u001b[1;32m   3349\u001b[0m \u001b[38;5;124;03mSee :class:`~torch.nn.MSELoss` for details.\u001b[39;00m\n\u001b[1;32m   3350\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target):\n\u001b[0;32m-> 3352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhandle_torch_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmse_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize_average\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize_average\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction\u001b[49m\n\u001b[1;32m   3354\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m   3356\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   3357\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3360\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   3361\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/overrides.py:1619\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1616\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1618\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[0;32m-> 1619\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpublic_api\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1621\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:3365\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3363\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3365\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (16) must match the size of tensor b (1600) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 10000\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    loss_value = loss_fun(t)\n",
    "    loss_value.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss_value.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
